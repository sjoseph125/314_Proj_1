---
title: 'Unit Project 3: Analysis Using simple and multiple linear regression '
authors: 'Samson Joseph, Sean Simmons, Daniel Gurwah, Joseph Difrancesco'
date: "5/10/2021"
output:
  html_document:
    df_print: paged
---
## Delete lines 8 - 16 section before submitting the file
This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook to use as a template for your Unit 2 Project. 

Add new chunks by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).  You may need to Knit to HTML, using the KNIT button on the toolbar. 

You will submit both the saved R Markdown file (.RMD) WITH A NEW NAME and the HTML file.
## Delete lines 8-16 before submitting the file.

**To:** DR. Random<br>

**From:** Samson Joseph, Sean Simmons, Daniel Gurwah, Christopher Aguila, Joseph Difrancesco<br>

**Subject:**Analysis of Video Games Dataset Using Linear Regression 


## Background
[Provide a brief (one to two paragraphs, please, that 1) recaps the key features of your dataset, 2) states your task as a follow-up of your previous data exploration.  You may use a table to summarize your dataset features, if you wish.]
```{r setup, include =FALSE}
#Loading Libraries
#install.packages('GGally')
#install.packages('psych')
library(tidyverse)
library(GGally)
library(psych)
library(car)
library(statsr)
library (pwr)

#Loading Dataset
vgs <- read.csv('replaced variables.csv')

#Transforming variables to appropriate data types
vgs$NA_Sales<-as.numeric(vgs$NA_Sales)
vgs$EU_Sales<-as.numeric(vgs$EU_Sales)
vgs$JP_Sales<-as.numeric(vgs$JP_Sales)
vgs$Other_Sales<-as.numeric(vgs$Other_Sales)
vgs$Platform <- as.factor(vgs$Platform)
vgs$Publisher <- as.factor(vgs$Genre)
vgs$Genre <- as.factor(vgs$Genre)
vgs$Developer <- as.factor(vgs$Developer)
vgs$Rating <- as.factor(vgs$Rating)
vgs$Year_of_Release <- as.factor(vgs$Year_of_Release)
vgs$User_Score <- as.numeric(vgs$User_Score)
vgs$Name <- as.character(vgs$Name)
vgs$User_Score <- vgs$User_Score * 10

#Replacing empty entries with NA
vgs$Rating[vgs$Rating %in% c("")]<- NA
vgs$Rating <-factor(vgs$Rating)
vgs$Developer[vgs$Developer %in% c("")]<- NA
vgs$Developer <-factor(vgs$Developer)
vgs$Genre[vgs$Genre %in% c("")]<- NA
vgs$Genre <-factor(vgs$Genre)
vgs <- filter(vgs, Name != "")
```

## Analyses

### Analysis 1: [Name/Description]
#### Analyst:[Name of Team Member]

In this analysis, I am testing to see whether there is an association between the age of the intended audience of a video game and the platform it is released on. I have simplified the originally reported ESRB ratings for the video games in the data set by fitting them into new categories labeled as "kids", "teens", and "adults" in order to make the results of the test more readable and practical. I've also removed all observations where the rating is listed as "RP", or Rating Pending. The most appropriate statistical test to measure the association between two categorical variables is a Chi-square test of independence.

In order to determine if the Chi-square test was a viable option to analyze this data, I first needed to make sure the data meets all of the conditions. The data meets the condition of independence as each observation is independent of all the others. Some data wrangling was required to make this true. I chose to completely remove all observations which had duplicate names. Some of these entries seemed to be completely erroneous, where there were two or more entires which were exact copies. It's also common for single games to be released on multiple platforms. I have eliminated all cases where this is true, opting instead to keep only those which were intended for only one platform. The data meet the sample size condition as each category has more than 5 cases. In this case, the condition is far exceeded, with the lowest number of cases being 54 (PC games made for an adult audience) and the highest being 2176 (handheld games made for kids). The degrees of freedom is determined by the number of rows and columns in our data set, with the formula (R-1)x(C-1). The data frame consists of 3 rows and 3 columns, therefore the degrees of freedom for this test is 4. This number is corroborated by the results of our chi-squared test in R.

The null hypothesis states that the rating and platform are independent - the rating does not vary by platform, and our alternative hypothesis is that rating and platform are dependent - the rating does vary by platform. After running the Chi-squared test on our contingency table, I got a p-value of p < 2.2e-16, which is very large, and therefore we will not reject the null hypothesis. Based on our test, there is no evidence that the rating of a video game has any effect on the platform that it gets released on.

```{r}
# copy to a new working dataset
vgs_chi <- vgs

# drop observations from the new dataset that have a duplicate name
vgs_chi <- vgs_chi[!(duplicated(vgs_chi$Name) | duplicated(vgs_chi$Name, fromLast = TRUE)), ]

# filter out all obvservations which show a rating on RP, and drop that level entirely
vgs_chi <- filter(vgs_chi, Rating != "RP") %>% droplevels()

# select only rating and platform from the dataset
vgs_chi <- select(vgs_chi, Rating, Platform)

vgs_chi <- na.omit(vgs_chi)

# combine all platforms relating console products 
levels(vgs_chi$Platform)[levels(vgs_chi$Platform)%in%c("Wii", "NES", "X360", "PS3", "PS2", "SNES", "PS4", "N64", "PS", "XB", "2600", "XOne", "WiiU", "GC", "GEN", "DC", "SAT", "SCD", "NG", "TG16", "3DO", "PCFX")] <- "console"

# combine all platforms relating handheld products
levels(vgs_chi$Platform)[levels(vgs_chi$Platform)%in%c("GB", "DS", "GBA", "3DS", "PSP", "PSV", "WS", "GG")] <- "handheld"

# combine all ratings intended for ages under 18
levels(vgs_chi$Rating)[levels(vgs_chi$Rating)%in%c("E", "E10+", "K-A", "EC")] <- "kids"

# combine all ratings intended for teens
levels(vgs_chi$Rating)[levels(vgs_chi$Rating)%in%c("T")] <- "teens"

# combine all ratings intended for ages over 18
levels(vgs_chi$Rating)[levels(vgs_chi$Rating)%in%c("M", "AO")] <- "adults"

# view / inspect dataframe
view(vgs_chi)
unique(vgs_chi)
summary(vgs_chi)

# contingency table with new variables
chi_t <- table(vgs_chi$Rating, vgs_chi$Platform)

# check contingency table
chi_t

# create mosaic plot using the data in the contingency table
mosaicplot(chi_t, main = "Intended Audience Accross Platforms",ylab = "Platform", xlab = "Age Groups")

# perform chi-squared test
chisq.test(chi_t)

```



### Analysis 2: Analysis of Variance Test
#### Analyst:Daniel Gurwah

In this analysis I will be performing an Analysis of Variance (ANNOVA) test to find if there is a difference in the means across the groups in the ratings category. I will be using the global sales variable as the quantitative or the response variable and the ratings will be used as the categorical or explanatory variable. To use the ratings I combined some of the categories in order to have less overlap of similar factors. I combined all of the everyone categories together, all of the adult categories together, and left everything else how they were. 

To perform an ANNOVA test, first a Levene test is ran to check the homogeniety of the variance between treatment groups and a QQNorm graph is ran to check the normality of each treatment group. For a Levene test the null hypothesis is there is no difference between treatment groups. The p-value of 9.527e-15 is very small and therefore we reject the null hypothesis. This means that there are differences between the treatment groups. After running the qqnorm graphs, it can be seen that in all of the groups besides one they are reflective of an exponential type of curve. This means that there is most likely not normality in the groups. 

**Null Hypothesis:** The mean between all treatment groups are the same. 

**Alternative Hypothesis:** The mean is different for at least one of the treatment groups.

After running the ANNOVA test the p-value is 2.53e-16 and the f-statistic is 0.007859374. Since the p-value is very small we will reject the null hypothesis. This means that there is strongly significant evidence of a difference between the different ratings and global sales.When running the power for each of these groups, they are all extremely low. For the everyone, mature, RP, and teen groups they have powers of: 0.14069527, 0.07371485, 0.05002809, 0.09672216. In order to obtain a power of at least 0.8 and avoid a type II error the groups should be 44126.79 total sales. The powers of these categories are so low due to the f-statistic being so low. In this case running a parametric test will be better since these variables did not meet the criteria for an ANNOVA test. 


```{r}
#copying the table for my variables of choice
vgs_copy <- select(vgs, Rating, Global_Sales)

#removing NA values
vgs_annova <- na.omit(vgs_copy)

#post transformation
table(vgs_annova$Rating)

vgs_annova <- mutate(vgs_annova, Rating = ifelse(Rating == "E" | Rating == "E10+" | Rating == "EC" | Rating == "K-A", "E",
                                                 ifelse(Rating == "AO" | Rating == "M", "M", 
                                                        ifelse(Rating == "RP", "RP",
                                                               ifelse(Rating == "T", "T",
                                                                      Rating)))))

#check
table(vgs_annova$Rating)

#checking conditions
leveneTest(vgs_annova$Global_Sales ~ vgs_annova$Rating)

tapply(vgs_annova$Global_Sales, vgs_annova$Rating, qqnorm)

#ANNOVA test
aov_test <- aov(data = vgs_annova, vgs_annova$Global_Sales ~ vgs_annova$Rating)
summary(aov_test)

#check differences between groups
TukeyHSD(aov_test)

#getting the f-stat
ef <- effectsize::eta_squared(aov_test)
ef$Eta2

#power tests
pwr.anova.test(k = 4, sig.level = 0.05, f = 0.007859374, n = c(5422, 1564, 3, 2961), power = NULL)

pwr.anova.test(k = 4, sig.level = 0.05, f = 0.007859374, n = NULL, power = .8)

```




### Analysis 3: Simple Linear Regression - EU_Sales and JP_Sales
#### Analyst: Joey diFrancesco

For this analysis, I am testing to see if Japan sales have any affect or impact on European sales. I chose EU_Sales to be my quantitative response variable and JP_Sales to be my quantitative explanatory variable. To do this analysis, Simple Linear Regression using two quantitative variables is what I found to be best fit. 

To proceed with linear regression, all conditions must be identified and met. The first condition is for the Y values or "errors" to be independent. To see if this is met, we must look at the Residual plot. The red line is fairly straight with no pattern and abnormalities, therefore the condition is met. The next condition is for the relationship between the explanatory and response variables must be linear. In our scatter plot, the data can be shown using a straight line, therefore the relationship is linear and the condition is met. For our last condition, the data must be normally distributed. We must look at our Normal Q-Q plot and see that the line associated with the y values are on a slight diagonal and straight. This shows that what we expect the residual errors should be are normally distributed, therefore the condition is met. 

The null hypothesis for this analysis would be that the response variable is not independent and is affected by the explanatory variable, or EU_Sales is affected by JP_Sales. The alternative hypothesis would be that the response variable is independent of the explanatory variable, or EU_Sales is not affected by JP_Sales. My test statistic is t = 27.691, and p-value < 2.2e-16. Considering the p-value is extremely small and less than 0.05, we reject the null hypothesis. The R^2 value I got is 0.232 or 23.2%. This value is on the lower side, and can mean not many data points fall within the results of the line formed by the regression equation. In our case, it is more difficult to predict human behavior (buying video games) so therefore the R^2 value will be low. 

In conclusion, my analysis shows that Japan sales have little to no affect on European sales considering the null hypothesis was rejected.

```{r}
#Retrieving summary stats of both variables
describe(vgs$EU_Sales)
describe(vgs$JP_Sales)

#Scatterplot of both variables and adding a regression line to the plot
ggplot(vgs, aes(x = JP_Sales, y = EU_Sales )) + xlim(0, 12) + ylim(0,15) +
    geom_point() + geom_smooth(method=lm, se=FALSE)

#Finding the correlation of the two variables with a 95% confidence level
cor.test(vgs$JP_Sales, vgs$EU_Sales, method = "pearson", conf.level = 0.95)

#Running the regression and calling the linear model mod
mod <- lm(vgs$EU_Sales ~ vgs$JP_Sales)
summary(mod)

#Determining with plots whether the data meets the conditions for a linear model
plot(mod)
```




### Analysis 4:[Multiple Linear Regression with EU_Sales, NA_Sales, and JP_Sales]
#### Analyst:[Christopher Aguila]

The quantitative response variable used in this analysis is EU_sales, and the
quantitative explanatory variables are NA_SALES and JP_Sales. The null
hypothesis of this analysis would be to obtain correlation coefficients equal
to 0 while the alternative hypothesis of this analysis would be to obtain
correlation coefficients that are not equal to 0.

After running the correlation test and visualizing the trends between the
response and explanatory variables through the use of scatter plots below, there
is some linearity present. The scatter plots themselves do not provide much
insight on the linearity but that is not true about the correlation test.
The results of the correlation table show a strong correlation between EU_Sales
and NA_Sales at 0.87 and a moderate correlation between EU_sales and NA_Sales at 0.50. The visualizations after running the regression show a normal distribution
and some linearity through the histogram and qqnorm plots with the model's
residuals. This satisfies the conditions of linearity, normality, and equal
variance for the most part.

The results of the regression model show an estimated coefficient of 0.57 for
NA_Sales and an estimated coefficient of 0.34 for JP_Sales. There also is an F-
statistic of 2788 present. With both of these estimates being considerably
different than 0, the high F statistic, and p-values for each respective value
being well below the significance level of 0.05, I can reject the null hypothesis
in this analysis. 

The adjusted R^2 for this analysis is 0.7679 which is moderately high, pointing
to many data points falling below the regression line equation. Based on all
these calculations in this analysis, I can conclude that North American sales
and Japanese sales have somewhat of an effect on European Sales.


```{r}
#Summary stats using Psych of specific variables
describe(vgs$EU_Sales)
describe(vgs$NA_Sales)
describe(vgs$JP_Sales)

# Select and identify quantitative response variable and at least two explanatory quantitative variables: The response variable will be EU sales while the explanatory variables will be NA sales and JP sales.

# Boxplot and histogram of EU Sales
ggplot(data=vgs) +
  geom_boxplot(mapping=aes(x=EU_Sales))
ggplot(data = vgs) +
  geom_histogram(mapping = aes(x = EU_Sales), binwidth = 0.5)

# Show any transformations done to your variables.
vgs_comp <- vgs[complete.cases(vgs), ]

#visualization for trends
plot(vgs_comp$EU_Sales~vgs_comp$NA_Sales, xlim=c(0,.47), ylim=c(0,.27))
plot(vgs_comp$EU_Sales~vgs_comp$JP_Sales, xlim=c(0,.47), ylim=c(0,.27))

#Create table of correlation and covariance for your variables.
vgsres <- vgs_comp %>%
  select_if(is.numeric)%>%
  cor()
round(vgsres, 2)

#Run The regression.
model <- lm(EU_Sales ~ NA_Sales + JP_Sales, data = vgs_comp)
summary(model)

# Determine whether your data meets the conditions for a linear model. State the conditions for a linear model. Perform all appropriate diagnostics. Show any calculations or visualizations that you use to justify this.
plot(model$residuals ~ vgs_comp$EU_Sales, xlim=c(0,.5), ylim=c(-1,.5))
abline(h = 0, lty = 3)
hist(model$residuals)
qqnorm(model$residuals)
qqline(model$residuals)

```




### Analysis 5: Multiple Regression with EU_Sales, NA_Sales and Platform
#### Analyst: Samson Joseph

The response variable used in this analysis is EU_Sales. And the explanatory variables are NA_Sales, the quantitative variable, and Platform, the categorical variable. The Platform variable has been simplified to only include the PS4 and XOne values. The conditions for conducting the regression model are discussed below, close to the necessary graphs to visualize the information. The null hypothesis would be getting correlation coefficients that are equal to 0. The alternative hypothesis would be getting correlation coefficients that are not equal to 0. After running the model, we get an estimated coefficient of 0.79071 for NA_Sales and -0.33644 for Platform. Both coefficients are meaningfully different from 0. And their respective p-values are considerably less than any significance level. Additionally, with an F statistic of 236.1 and a p-value which is considerably less than .05, we can confidently reject the null hypothesis. The resulting adjusted R^2 value is only .516 which is not very high. This tells us that only 51.6% of the variation in the EU_Sales data can be explained by the regression model. The other 48.4% of the variation is a result of variables not considered for the regression or is a result of pure randomness. The linear equation EU_Sales = 0.22654 + 0.79071 * NA_Sales -.33644 * (1 for XOne and 0 for PS4) can be explained as such. This model can predict that for every x amount of sales in North America, sales in the EU will be .79071 of the x plus .22654. Additionally if the game is for XOne, the sales will be .3364 less and if the game is for PS4 then the sales will be .3364 more. With an R^2 of .516, this method can only explain 51.6% of the variation, so other variables must be considered for a better linear model.

In conclusion, the conditions for linear modeling is mostly met, and the resulting p-values for the estimated coefficients and F statistic are all well below that of any significance level. Therefore, the null hypothesis of zero correlation between the variables EU_Sales, NA_Sales, and Platform is rejected.



```{r warning=FALSE}

NA_sales_A5 <- data.frame(NA_Sales=vgs$NA_Sales[!is.na(vgs$NA_Sales)])
EU_Sales_A5 <- data.frame(EU_Sales=vgs$EU_Sales[!is.na(vgs$EU_Sales)])

#Showing the distribution of EU_Sales
#A limit of .27 was applied to the y axis of the boxplot and the x axis of the histogram to the data without the outliers. 0.27 was calculated by multiplying the IQR of EU_Sales, which is .18 by 1.5.

ggplot(EU_Sales_A5,aes(x='',EU_Sales)) + geom_boxplot() + ylim(0,.27) +ylab('EU Sales')

ggplot(EU_Sales_A5, aes(x=EU_Sales)) + geom_histogram(binwidth = .01) + xlim(.01,.27)

describe(vgs$EU_Sales)
```


The graph and table below help visualize the linearity of the variables, EU_Sales and NA_Sales. With a correlation of .68, they do have a linear relationship.

```{r warning=FALSE}
#Created a new DataFrame that includes EU_Sales, the response variable, and NA_Sales and Platform, the explanatory variables.
A5 <- vgs %>%
  select(EU_Sales, NA_Sales, Platform) %>%
  filter(Platform == "PS4" | Platform == "XOne")
A5<-na.omit(A5)

#Making a scatter plot to compare EU_Sales with NA_Sales in order to visualize any trends.
plot(A5$EU_Sales~A5$NA_Sales, xlim=c(0,.47))
plot(A5$EU_Sales~A5$NA_Sales, xlim=c(0,.47), ylim=c(0,.27)) #.27

#Creating a DataFrame to analyze the correlation of EU_Sales and NA_Sales
corT <- data.frame(A5 %>%
  select_if(is.numeric) %>%
  cor())
corT

```
The first and second graph in below section give visualizations on the residuals of the model. The first plot shows a good scatter among the residuals. Although not the ideal spread, it is enough to satisfy the condition of the residual independence. In order to check the normality of the residuals we will use a histogram and a qqnorm graph. The histogram shows a bimodal distribution and the qqnorm is relatively satisfactory, with some quirkiness at the ends. The bimodal distribution is not too severe, but is mostly satisfactory. But homoscedasticity of the residuals is a bit more questionable as there is a meaningful amount below the 0 line than above.

```{r warning=FALSE}

#Creating a multi-linear regression model
Mlm <- model <- lm(EU_Sales ~ NA_Sales + Platform, data = A5)
summary(Mlm)

#Plotting the residuals to visualize their variability
plot(Mlm$residuals ~ A5$EU_Sales,xlim=c(0,.5), ylim=c(-1,.5))
abline(h = 0, lty = 3)
hist(Mlm$residuals, breaks = 50)

qqnorm(Mlm$residuals)
qqline(Mlm$residuals)

leveragePlots(Mlm, xlim=c(-.5,.5))

sigma(Mlm)/mean(A5$EU_Sales)

anova(Mlm)

```





## Recommendations

[Add text section with your recommendations here. In this section, you will briefly  summarize the conclusions of your estimates and tests. What statistically significant findings did you have?  What limitations are there in your findings?  Discuss if/how this is relevant in the context of your data – does it give us new insights ? Help us to better understand some phenomenon?  Or not?]

## Reflections
[Add text section on your reflections about unit projects. Be sure to address each of the prompts on the instruction sheet.]